{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2fd3d6b-cedb-4070-84db-9b1d09a0f271",
   "metadata": {},
   "source": [
    "# CHEME 5760: Take Home Prelim 1 (THP1) Fall 2023\n",
    "Take Home Prelim 1 (THP1) aims to test the hypothesis that the `apples and oranges problem` can be structured as a `Markov Decision Process (MDP)`, where an optimal policy can be computed using `value iteration`. Toward this objective, there are two problems:\n",
    "\n",
    "* `Problem 1`: In problem 1, we solve the `apples and oranges problem` subject to a budget constraint as a non-linear programming problem. The optimal solution of this problem will be used as the `terminal state` for the `MDP` calculations\n",
    "* `Problem 2`: Construct an `MDP` problem encoding the `apple and oranges` decision, and solve for the optimal policy using `value iteration`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad5d878-10d4-4af8-85fa-21c55cde92fa",
   "metadata": {},
   "source": [
    "## Setup\n",
    "The computations in `THP1` are enabled by several external `Julia` packages and custom codes the teaching team has developed to work with these packages. To include this code, we [include](https://docs.julialang.org/en/v1/manual/code-loading/) the `Include.jl` file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25550134-a286-4aa1-8aee-19a7bd80d1d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e777da94-8a5f-45e6-bf2e-ec8abee6fee1",
   "metadata": {},
   "source": [
    "The `U(...)` function computes the utility of combinations of `apples` and `oranges` using a `Cobb-Douglas` utility function. A tuple holding the number of `(apples, oranges)` and the $\\alpha$-vector are passed as arguments; the `utility` value is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ca18d71-b870-4cd1-a70b-821265f5de38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function U(x::Tuple{Int,Int}, α::Array{Float64,1})::Float64\n",
    "    \n",
    "    # get the apples, and oranges \n",
    "    apples = x[1];\n",
    "    oranges = x[2];\n",
    "    \n",
    "    # compute the objective -\n",
    "    utility = (apples^α[1])*(oranges^α[2]);\n",
    "    \n",
    "    # return -\n",
    "    return utility;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca9647f-0576-4a9e-81bf-b01c0ec4ade0",
   "metadata": {},
   "source": [
    "## `Problem 1`: Compute the optimal number of `Apples` and `Oranges` to purchase\n",
    "Use a `Cobb-Douglas` utility function combined with a budget constraint to compute the optimal combination of `apples` and `oranges`. Let's assume we have the following parameters:\n",
    "\n",
    "* The coefficient vector $\\alpha = (0.55,0.45)$ and the total budget is `50 USD`\n",
    "* The unit price of an `apple` is `0.98 USD` and the unit price of an `orange` is `1.49 USD`\n",
    "* Let `apples` be index `1` and `oranges` be index `2`\n",
    "* Assume the bounds run from `0 to 100` and the initial guess is `0.1*ones(2)`.\n",
    "\n",
    "### Strategy\n",
    "* Use the `build(...)` to construct an instance of the `MySimpleCobbDouglasChoiceProblem` type holding the lecture parameters, and set this to the `base_problem` variable.\n",
    "* Pass the `base_problem` problem to the `solve(...)` function, set the return to the variable `base_solution`.\n",
    "\n",
    "For help with `problem 1`, check out `Lab 3c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28507283-c742-476c-a301-404d18c21598",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching solve(::Nothing)\n\n\u001b[0mClosest candidates are:\n\u001b[0m  solve(\u001b[91m::MySimpleCobbDouglasChoiceProblem\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mMain\u001b[39m \u001b[90m~/Desktop/julia_work/CHEME-5760-THP1-Fall-2023/src/\u001b[39m\u001b[90m\u001b[4mSolve.jl:1\u001b[24m\u001b[39m\n\u001b[0m  solve(\u001b[91m::MyValueIterationModel\u001b[39m, \u001b[91m::MyMDPProblemModel\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mMain\u001b[39m \u001b[90m~/Desktop/julia_work/CHEME-5760-THP1-Fall-2023/src/\u001b[39m\u001b[90m\u001b[4mSolve.jl:40\u001b[24m\u001b[39m\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching solve(::Nothing)\n\n\u001b[0mClosest candidates are:\n\u001b[0m  solve(\u001b[91m::MySimpleCobbDouglasChoiceProblem\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mMain\u001b[39m \u001b[90m~/Desktop/julia_work/CHEME-5760-THP1-Fall-2023/src/\u001b[39m\u001b[90m\u001b[4mSolve.jl:1\u001b[24m\u001b[39m\n\u001b[0m  solve(\u001b[91m::MyValueIterationModel\u001b[39m, \u001b[91m::MyMDPProblemModel\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mMain\u001b[39m \u001b[90m~/Desktop/julia_work/CHEME-5760-THP1-Fall-2023/src/\u001b[39m\u001b[90m\u001b[4mSolve.jl:40\u001b[24m\u001b[39m\n",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[3]:10"
     ]
    }
   ],
   "source": [
    "# initialize -\n",
    "α = [0.55, 0.45]; # coefficients\n",
    "c = [0.98, 1.49]; # price of x1 and x2\n",
    "total_budget = 50.0;\n",
    "\n",
    "# build my problem object -\n",
    "base = nothing\n",
    "\n",
    "# call the solve function. This will return a dictionary -\n",
    "base_solution = solve(base);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caaeb24b-0d4f-491e-83b3-ee11e9c00190",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `base_solution` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `base_solution` not defined",
      ""
     ]
    }
   ],
   "source": [
    "base_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6985ab-a6ef-4b4f-a320-371c140bde07",
   "metadata": {},
   "source": [
    "Describe me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54f99fa0-70e2-4969-80f4-d5eeb4fd9ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `base_solution` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `base_solution` not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[5]:1"
     ]
    }
   ],
   "source": [
    "optimal_apples = base_solution[\"argmax\"][1] |> x-> round(x,digits=0) |> Int\n",
    "optimal_oranges = base_solution[\"argmax\"][2] |> x-> round(x,digits=0) |> Int\n",
    "println(\"(apples, oranges) = ($(optimal_apples),$(optimal_oranges))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939af937-2f6d-4a17-add7-e50909c87961",
   "metadata": {},
   "source": [
    "## `Problem 2`: Poise the `Apples` and `Oranges` problem as an MDP and solve using Value Iteration\n",
    "\n",
    "* __Task 1__: Setup a $30\\times{30}$ grid, encoded this model as an instance of the `MyRectangularGridWorldModel` type\n",
    "      * `TODO`: Add a `terminal state` at the optimal combination of `apples` and `oranges` computed from `problem 1`. Set the reward for this state to be the optimal _integer_ fitness value, using the `U(...)` function.\n",
    "  * `TODO`: Add the optimal combination of `apples` and `oranges` computed from `problem 1` to the `absorbing_state_set`  \n",
    "* __Task 2__: Using your `MyRectangularGridWorldModel` instance to generate the components of the `MDP`, namely, the reward function (or array) `R(s, a)`, and the model of the physics of the world in the transition function (or array) `T(s, s′, a)`.\n",
    "    * `TODO`: Modify the $R[s,a]$ array from `PS5` so that it uses the `U(...)` function for the default values \n",
    "    * `TODO`: Modify the $R[s, a]$ array so that it describes a `soft wall`, i.e., a region where the budget constraint is violated. Set the `wall` penalty as `-1000`. \n",
    "* __Task 3__: For $\\gamma = 0.95\\times{k}_{\\max}=250$ use `value iteration` to estimate the optimal value function $U^{\\star}(s)$. \n",
    "    * `TODO`: For ($\\gamma$,$k_{\\max}$), extract the `action-value function` $Q(s, a)$ from the optimal optimal value function $U^{\\star}(s)$, and compute the optimal navigation policy $\\pi^{\\star}(s)$ from $Q(s,a)$\n",
    "    * `TODO`: Develop an approach to visualize the optimal policy.\n",
    "    \n",
    "For help with `problem 2`, check out the solution to `PS5`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08abf7f-ed7d-4a72-be89-ddb403e1bc06",
   "metadata": {},
   "source": [
    "### Task 1: Build the `Apples` and `Oranges` world model\n",
    "We encoded the `rectangular grid world` using the `MyRectangularGridWorldModel` model, which we construct using a `build(...)` method. Let's set up the data for the `apples` and `oranges` world, i.e., set up the states, actions, and rewards and then construct the world model. \n",
    "* First, set values for the `number_of_rows` and `number_of_cols` variables, the `nactions` available to the agent, and the `discount factor` $\\gamma$. \n",
    "* Then, we'll compute the number of states and set up the state set $\\mathcal{S}$ and the action set $\\mathcal{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "656b80a2-e389-49ed-ae8b-8e69b164d4a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: Cannot construct range from arguments:\nstart = 1\nstep = 1\nstop = nothing\nlength = nothing\nTry specifying more arguments.\n",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Cannot construct range from arguments:\nstart = 1\nstep = 1\nstop = nothing\nlength = nothing\nTry specifying more arguments.\n",
      "",
      "Stacktrace:",
      " [1] range_error(start::Int64, step::Int64, stop::Nothing, length::Nothing)",
      "   @ Base ./range.jl:244",
      " [2] _range(start::Int64, step::Int64, stop::Nothing, len::Nothing)",
      "   @ Base ./range.jl:162",
      " [3] range(start::Int64; stop::Nothing, length::Nothing, step::Int64)",
      "   @ Base ./range.jl:142",
      " [4] top-level scope",
      "   @ In[6]:5"
     ]
    }
   ],
   "source": [
    "number_of_rows = nothing # TODO: fill me in\n",
    "number_of_cols = nothing # TODO: fill me in\n",
    "nactions = nothing; # TODO: fill me in\n",
    "nstates = nothing; # TODO: fill me in\n",
    "𝒮 = range(1,stop=nstates,step=1) |> collect;\n",
    "𝒜 = range(1,stop=nactions,step=1) |> collect;\n",
    "γ = 0.95;\n",
    "k_max = 250;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ba399-75e0-4be9-88a9-e973151aa607",
   "metadata": {},
   "source": [
    "Next, set up a description of the rewards, the `rewards::Dict{Tuple{Int,Int}, Float64}` dictionary, which maps the $(x,y)$-coordinates to a reward value. \n",
    "  * `TODO`: Add a `terminal state` at the optimal combination of `apples` and `oranges` computed from `problem 1`. Set the reward for this state to be the optimal _integer_ fitness value, using the `U(...)` function.\n",
    "  * `TODO`: Add the optimal combination of `apples` and `oranges` computed from `problem 1` to the `absorbing_state_set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a53a314-6fa1-4ac3-addd-36b1aad0076c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `optimal_apples` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `optimal_apples` not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[7]:3"
     ]
    }
   ],
   "source": [
    "my_objective_value = nothing; # TODO: call the U(...) function\n",
    "rewards = Dict{Tuple{Int,Int}, Float64}()\n",
    "rewards[(optimal_apples, optimal_oranges)] = my_objective_value;\n",
    "\n",
    "# setup set of absorbing states -\n",
    "absorbing_state_set = Set{Tuple{Int,Int}}()\n",
    "push!(absorbing_state_set, (optimal_apples, optimal_oranges));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc44527-806d-442a-b915-42a81d06890d",
   "metadata": {},
   "source": [
    "Finally, build an instance of the `MyRectangularGridWorldModel` type, which models the grid world. We save this instance in the `world` variable\n",
    "* Pass in the number of rows `nrows`, number of cols `ncols`, and our initial reward description in the `rewards` field into the `build(...)` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a91e120-7ca3-48cf-8ee3-ff62ae899c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching (::Colon)(::Int64, ::Nothing)\n\n\u001b[0mClosest candidates are:\n\u001b[0m  (::Colon)(::T, ::Any, \u001b[91m::T\u001b[39m) where T<:Real\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m\u001b[4mrange.jl:45\u001b[24m\u001b[39m\n\u001b[0m  (::Colon)(::A, ::Any, \u001b[91m::C\u001b[39m) where {A<:Real, C<:Real}\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m\u001b[4mrange.jl:10\u001b[24m\u001b[39m\n\u001b[0m  (::Colon)(::T, ::Any, \u001b[91m::T\u001b[39m) where T\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m\u001b[4mrange.jl:44\u001b[24m\u001b[39m\n\u001b[0m  ...\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching (::Colon)(::Int64, ::Nothing)\n\n\u001b[0mClosest candidates are:\n\u001b[0m  (::Colon)(::T, ::Any, \u001b[91m::T\u001b[39m) where T<:Real\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m\u001b[4mrange.jl:45\u001b[24m\u001b[39m\n\u001b[0m  (::Colon)(::A, ::Any, \u001b[91m::C\u001b[39m) where {A<:Real, C<:Real}\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m\u001b[4mrange.jl:10\u001b[24m\u001b[39m\n\u001b[0m  (::Colon)(::T, ::Any, \u001b[91m::T\u001b[39m) where T\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m\u001b[4mrange.jl:44\u001b[24m\u001b[39m\n\u001b[0m  ...\n",
      "",
      "Stacktrace:",
      " [1] build(modeltype::Type{MyRectangularGridWorldModel}, data::NamedTuple{(:nrows, :ncols, :rewards), Tuple{Nothing, Nothing, Dict{Tuple{Int64, Int64}, Float64}}})",
      "   @ Main ~/Desktop/julia_work/CHEME-5760-THP1-Fall-2023/src/Factory.jl:51",
      " [2] top-level scope",
      "   @ In[8]:1"
     ]
    }
   ],
   "source": [
    "world = build(MyRectangularGridWorldModel, \n",
    "    (nrows = number_of_rows, ncols = number_of_cols, rewards = rewards));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf02232-2971-48a3-a8a1-90e5619d1299",
   "metadata": {},
   "source": [
    "### Task 2: Generate the components of the MDP problem\n",
    "The MDP problem requires the return function (or array) `R(s, a)`, and the transition function (or array) `T(s, s′, a)`. Let's construct these from our grid world model instance, starting with the `R(s, a)` reward function.\n",
    "\n",
    "#### Rewards $R(s,a)$\n",
    "We'll encode the reward function as a $\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ array, which holds the reward values for being in state $s\\in\\mathcal{S}$ and taking action $a\\in\\mathcal{A}$. After initializing the `R`-array and filling it with zeros, we'll populate the non-zero values of $R(s, a)$ using nested `for` loops. During each iteration of the `outer` loop, we'll:\n",
    "* Select a state `s`, an action `a`, and a move `Δ`\n",
    "* We'll then compute the new position resulting from implementing action `a` from the current position and store this in the `new_position` variable. * If the `new_position`$\\in\\mathcal{S}$ is in our initial `rewards` dictionary (the charging station or a lava pit), we use that reward value from the `rewards` dictionary. If we are still in the world but not in a special location, we set the reward to `-1`.\n",
    "* Finally, if `new_position`$\\notin\\mathcal{S}$, i.e., the `new_position` is a space outside the grid, we set a penalty of `-50000.0`.\n",
    "\n",
    "#### Modifications\n",
    "* `TODO`: Modify the $R[s,a]$ array from `PS5` so that it uses the `U(...)` function for the default values \n",
    "* `TODO`: Modify the $R[s, a]$ array to describe a `soft wall`, i.e., a region where the budget constraint is violated. Set the `wall` penalty as `-1000`. Allow up to a `1 USD` violation of the budget constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "288ab213-85ac-47b8-966e-a5a5d1e88706",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching zeros(::Nothing, ::Nothing)",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching zeros(::Nothing, ::Nothing)",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[9]:1"
     ]
    }
   ],
   "source": [
    "R = zeros(nstates, nactions);\n",
    "fill!(R, 0.0)\n",
    "for s ∈ 𝒮\n",
    "    for a ∈ 𝒜\n",
    "        \n",
    "        Δ = world.moves[a];\n",
    "        current_position = world.coordinates[s]\n",
    "        new_position =  current_position .+ Δ\n",
    "        \n",
    "        # TODO: fill me in\n",
    "    end\n",
    "end\n",
    "\n",
    "# setup soft walls -\n",
    "soft_wall_set = Set{Tuple{Int,Int}}();\n",
    "for s ∈ 𝒮\n",
    "    \n",
    "    # get the position -\n",
    "    current_position = world.coordinates[s]\n",
    "    \n",
    "    # TODO: current_position violate the budget? (with a 1 USD grace)?\n",
    "    # TODO: Hint: think about this like a penalty function\n",
    "    # if yes, store this position in the soft_wall_set\n",
    "end\n",
    "\n",
    "for s ∈ 𝒮\n",
    "    current_position = world.coordinates[s]\n",
    "    for a ∈ 𝒜\n",
    "        Δ = world.moves[a];\n",
    "        new_position =  current_position .+ Δ\n",
    "        \n",
    "        if (in(new_position, soft_wall_set) == true)\n",
    "          R[s,a] = -1000.0  \n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1164cc27-d821-469c-bb3d-0ada67b2ff0c",
   "metadata": {},
   "source": [
    "### Transition $T(s, s^{\\prime},a)$\n",
    "Next, build the transition function $T(s,s^{\\prime},a)$. We'll encode this as a $\\dim\\mathcal{S}\\times\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ [multidimension array](https://docs.julialang.org/en/v1/manual/arrays/) and populate it using nested `for` loops. \n",
    "\n",
    "* The `outer` loop we will iterate over actions. For every $a\\in\\mathcal{A}$ will get the move associated with that action and store it in the `Δ`\n",
    "* In the `inner` loop, we will iterate over states $s\\in\\mathcal{S}$. We compute a `new_position` resulting from implementing action $a$ and check if `new_position`$\\in\\mathcal{S}$. If `new_position` is in the world, and `current_position` is _not_ an `absorbing state` we set $s^{\\prime}\\leftarrow$`world.states[new_position]`, and `T[s, s′,  a] = 1.0`\n",
    "* However, if the `new_position` is outside of the grid (or we are jumping from an `absorbing` state), we set `T[s, s,  a] = 1.0`, i.e., the probability that we stay in `s` if we take action `a` is `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "923e659b-22c8-4a3e-ab86-077b91e489b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching Array{Float64, 3}(::UndefInitializer, ::Nothing, ::Nothing, ::Nothing)\n\n\u001b[0mClosest candidates are:\n\u001b[0m  Array{T, N}(\u001b[91m::Missing\u001b[39m, ::Any...) where {T, N}\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m\u001b[4mbaseext.jl:43\u001b[24m\u001b[39m\n\u001b[0m  Array{T, N}(\u001b[91m::Nothing\u001b[39m, ::Any...) where {T, N}\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m\u001b[4mbaseext.jl:42\u001b[24m\u001b[39m\n\u001b[0m  Array{T, 3}(::UndefInitializer, \u001b[91m::Tuple{Int64, Int64, Int64}\u001b[39m) where T\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mCore\u001b[39m \u001b[90m\u001b[4mboot.jl:488\u001b[24m\u001b[39m\n\u001b[0m  ...\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching Array{Float64, 3}(::UndefInitializer, ::Nothing, ::Nothing, ::Nothing)\n\n\u001b[0mClosest candidates are:\n\u001b[0m  Array{T, N}(\u001b[91m::Missing\u001b[39m, ::Any...) where {T, N}\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m\u001b[4mbaseext.jl:43\u001b[24m\u001b[39m\n\u001b[0m  Array{T, N}(\u001b[91m::Nothing\u001b[39m, ::Any...) where {T, N}\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m\u001b[4mbaseext.jl:42\u001b[24m\u001b[39m\n\u001b[0m  Array{T, 3}(::UndefInitializer, \u001b[91m::Tuple{Int64, Int64, Int64}\u001b[39m) where T\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mCore\u001b[39m \u001b[90m\u001b[4mboot.jl:488\u001b[24m\u001b[39m\n\u001b[0m  ...\n",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[10]:2"
     ]
    }
   ],
   "source": [
    "# --- DO NOT CHANGE ME .... STEP AWAY FROM YOUR KEYBOARD --------------- #\n",
    "T = Array{Float64,3}(undef, nstates, nstates, nactions);\n",
    "fill!(T, 0.0)\n",
    "for a ∈ 𝒜\n",
    "    \n",
    "    Δ = world.moves[a];\n",
    "    \n",
    "    for s ∈ 𝒮\n",
    "        current_position = world.coordinates[s]\n",
    "        new_position =  current_position .+ Δ\n",
    "        if (haskey(world.states, new_position) == true && \n",
    "                in(current_position, absorbing_state_set) == false)\n",
    "            s′ = world.states[new_position];\n",
    "            T[s, s′,  a] = 1.0\n",
    "        else\n",
    "            T[s, s,  a] = 1.0\n",
    "        end\n",
    "    end\n",
    "end\n",
    "# --------------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb5c6a-255b-41d0-aebd-4c8695bdbb95",
   "metadata": {},
   "source": [
    "## Task 3: For $\\gamma = 0.95\\times{k}_{\\max}=250$ use `value iteration` to estimate the optimal value function $U^{\\star}(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4f378-f223-4c44-9f17-abd0e41b6404",
   "metadata": {},
   "source": [
    "In Task 3, we construct an `MDP` and then solve the `MDP` using value iteration. The solution of the value iteration procedure is then used to estimate the optimal policy $\\pi^{\\star}(s)$. Toward this task, first, we construct an instance of the `MyMDPProblemModel`, which encodes the data required to solve the MDP problem.\n",
    "* We pass the states `𝒮`, the actions `𝒜`, the transition matrix `T`, the reward matrix `R`, and the discount factor `γ` into the `build(...)` method. We store the MDP model in the `m` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bee066c6-f228-4d4d-a9ad-079cb6995b28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `𝒮` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `𝒮` not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[11]:1"
     ]
    }
   ],
   "source": [
    "m = build(MyMDPProblemModel, (𝒮 = 𝒮, 𝒜 = 𝒜, T = T, R = R, γ = γ));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb505c65-f9a2-4e9a-874b-8a7b06f5aec5",
   "metadata": {},
   "source": [
    "Next, we call the `solve(...)` method by passing a `value_iteration_model` instance and our MDP model `m::MyMDPProblemModel` as arguments. The `solve(...)` method iteratively computes the optimal value function $U^{\\star}(s)$ and returns it in an instance of the `MyValueFunctionPolicy` type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e41d060-5c23-4e9f-9c11-af63fa3a33f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `k_max` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `k_max` not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[12]:1"
     ]
    }
   ],
   "source": [
    "value_iteration_model = MyValueIterationModel(k_max); # takes k_max as argument\n",
    "solution = nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f5d41c-b7c2-4ed4-bf8d-ed40a9eebdd7",
   "metadata": {},
   "source": [
    "Now, we extract the `action-value function` or $Q(s, a)$ from the optimal optimal value function $U^{\\star}(s)$. We can do this using the `Q(...)` function, which takes `m` and the `solution::MyValueFunctionPolicy`. Save the optimal $Q(s,a)$ in the `my_Q` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c091d3c-e3b9-4bd3-8195-8220d58fecce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `m` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `m` not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[13]:1"
     ]
    }
   ],
   "source": [
    "my_Q = Q(m, solution.U)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d8d92-5880-49d6-910d-68d99da2a1fb",
   "metadata": {},
   "source": [
    "Finally, compute the optimal navigation policy $\\pi^{\\star}(s)$ from $Q(s,a)$ using the `policy(...)` function. Save this in the `my_π` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6468d7ad-fac1-422b-b7f6-4cf7619a0544",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `my_Q` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `my_Q` not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[14]:1"
     ]
    }
   ],
   "source": [
    "my_π = policy(my_Q);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aaf91e-e201-43c2-8b84-d37a53df9c57",
   "metadata": {},
   "source": [
    "### `TODO` Visualize the optimal policy\n",
    "Let's use the visualization code from `PS5` to visualize the optimal policy $\\pi^{\\star}(s)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4edb0539-29cb-4b00-a58e-2b4a370765bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `world` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `world` not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[15]:6"
     ]
    }
   ],
   "source": [
    "# setup -\n",
    "# draw the path -\n",
    "p = plot();\n",
    "initial_site = (1,30)\n",
    "hit_absorbing_state = false\n",
    "s = world.states[initial_site];\n",
    "visited_sites = Set{Tuple{Int,Int}}();\n",
    "push!(visited_sites, initial_site);\n",
    "\n",
    "while (hit_absorbing_state == false)\n",
    "    current_position = world.coordinates[s]\n",
    "    a = my_π[s];\n",
    "    Δ = world.moves[a];\n",
    "    new_position =  current_position .+ Δ\n",
    "    scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, msc=:black, c=:blue)\n",
    "    plot!([current_position[1], new_position[1]],[current_position[2],new_position[2]], label=\"\", arrow=true, lw=2, c=:gray)\n",
    "    \n",
    "    if (in(new_position, absorbing_state_set) == true || in(new_position, visited_sites) == true)\n",
    "        hit_absorbing_state = true;\n",
    "    else\n",
    "        s = world.states[new_position];\n",
    "        push!(visited_sites, new_position);\n",
    "    end\n",
    "end\n",
    "\n",
    "# draw the grid -\n",
    "for s ∈ 𝒮\n",
    "    current_position = world.coordinates[s]\n",
    "    a = my_π[s];\n",
    "    Δ = world.moves[a];\n",
    "    new_position =  current_position .+ Δ\n",
    "    \n",
    "    if (haskey(rewards, current_position) == true && rewards[current_position] == my_objective_value)\n",
    "        scatter!([current_position[1]],[current_position[2]], label=\"$(current_position)\", showaxis=:false, c=:green, ms=4)\n",
    "    elseif (in(current_position, soft_wall_set) == true)\n",
    "        scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, c=:gray69, ms=4)\n",
    "    else\n",
    "        scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, msc=:black, c=:white)\n",
    "    end\n",
    "end\n",
    "xlabel!(\"Apples\",fontsize=18)\n",
    "ylabel!(\"Oranges\",fontsize=18)\n",
    "current()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
