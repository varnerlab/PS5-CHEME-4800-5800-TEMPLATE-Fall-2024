





include("Include.jl");





function U(x::Tuple{Int,Int}, Î±::Array{Float64,1})::Float64
    
    # get the apples, and oranges 
    apples = x[1];
    oranges = x[2];
    
    # compute the objective -
    utility = (apples^Î±[1])*(oranges^Î±[2]);
    
    # return -
    return utility;
end;





# initialize -
Î± = [0.55, 0.45]; # coefficients
c = [0.98, 1.49]; # price of x1 and x2
total_budget = 50.0;

# build my problem object -
base = build(MySimpleCobbDouglasChoiceProblem, (
    
    initial = 0.1*ones(2), # initial guess
    Î± = Î±, # coefficients
    c = c, # price of x1 and x2
    I = total_budget, # income
    
    # how much of xâ‚ and xâ‚‚ can be we buy?
    bounds = [
        0.0 100.0; # L U
        0.0 100.0; # L U
    ]
));

# call the solve function. This will return a dictionary -
base_solution = solve(base);


base_solution





optimal_apples = base_solution["argmax"][1] |> x-> round(x,digits=0) |> Int
optimal_oranges = base_solution["argmax"][2] |> x-> round(x,digits=0) |> Int
println("(apples, oranges) = ($(optimal_apples),$(optimal_oranges))")








number_of_rows = 30
number_of_cols = 30
nactions = 4;
nstates = (number_of_rows*number_of_cols);
ğ’® = range(1,stop=nstates,step=1) |> collect;
ğ’œ = range(1,stop=nactions,step=1) |> collect;
Î³ = 0.95;





my_objective_value = U((optimal_apples, optimal_oranges), Î±)
rewards = Dict{Tuple{Int,Int}, Float64}()
rewards[(optimal_apples, optimal_oranges)] = my_objective_value;

# setup set of absorbing states -
absorbing_state_set = Set{Tuple{Int,Int}}()
push!(absorbing_state_set, (optimal_apples, optimal_oranges));





world = build(MyRectangularGridWorldModel, 
    (nrows = number_of_rows, ncols = number_of_cols, rewards = rewards));





R = zeros(nstates, nactions);
fill!(R, 0.0)
for s âˆˆ ğ’®
    for a âˆˆ ğ’œ
        
        Î” = world.moves[a];
        current_position = world.coordinates[s]
        new_position =  current_position .+ Î”
        
        if (haskey(world.states, new_position) == true)
            if (haskey(rewards, new_position) == true)
                R[s,a] = rewards[new_position];
            else
                R[s,a] = U(new_position, Î±);
            end
        else
            R[s,a] = -50000.0; # we are off the grid, big negative penalty
        end
    end
end

# setup soft walls -
soft_wall_set = Set{Tuple{Int,Int}}();
for s âˆˆ ğ’®
    
    # get the position -
    current_position = world.coordinates[s]
    
    # does this position violate the budget?
    budget_violation = max(0.0, c[1]*current_position[1]+c[2]*current_position[2] - total_budget)
    if (budget_violation â‰¥ 1.0)
        push!(soft_wall_set, current_position)
    end
end

for s âˆˆ ğ’®
    current_position = world.coordinates[s]
    for a âˆˆ ğ’œ
        Î” = world.moves[a];
        new_position =  current_position .+ Î”
        
        if (in(new_position, soft_wall_set) == true)
          R[s,a] = -1000.0  
        end
    end
end





T = Array{Float64,3}(undef, nstates, nstates, nactions);
fill!(T, 0.0)
for a âˆˆ ğ’œ
    
    Î” = world.moves[a];
    
    for s âˆˆ ğ’®
        current_position = world.coordinates[s]
        new_position =  current_position .+ Î”
        if (haskey(world.states, new_position) == true && 
                in(current_position, absorbing_state_set) == false)
            sâ€² = world.states[new_position];
            T[s, sâ€²,  a] = 1.0
        else
            T[s, s,  a] = 1.0
        end
    end
end





m = build(MyMDPProblemModel, (ğ’® = ğ’®, ğ’œ = ğ’œ, T = T, R = R, Î³ = Î³));





solution = let

    k_max = 250; # maximum number of iterations
    value_iteration_model = MyValueIterationModel(k_max); # takes k_max as argument
    solution = solve(value_iteration_model,m); # TODO: update me!

    # solution -
    solution
end





my_Q = Q(m, solution.U)





my_Ï€ = policy(my_Q)





initial_site = (1,10); # horizontal, vertical


let
    # draw the path -
    p = plot();
    hit_absorbing_state = false
    s = world.states[initial_site];
    visited_sites = Set{Tuple{Int,Int}}();
    push!(visited_sites, initial_site);
    
    while (hit_absorbing_state == false)
        current_position = world.coordinates[s]
        a = my_Ï€[s];
        Î” = world.moves[a];
        new_position =  current_position .+ Î”
        scatter!([current_position[1]],[current_position[2]], label="", showaxis=:false, msc=:black, c=:blue)
        plot!([current_position[1], new_position[1]],[current_position[2], new_position[2]], label="", arrow=true, lw=1, c=:red)
        
        if (in(new_position, absorbing_state_set) == true || in(new_position, visited_sites) == true)
            hit_absorbing_state = true;
        else
            s = world.states[new_position];
            push!(visited_sites, new_position);
        end
    end
    
    # draw the grid -
    for s âˆˆ ğ’®
        current_position = world.coordinates[s]
        a = my_Ï€[s];
        Î” = world.moves[a];
        new_position =  current_position .+ Î”
        
        if (haskey(rewards, current_position) == true && rewards[current_position] == my_objective_value)
            scatter!([current_position[1]],[current_position[2]], label="Optimal: $(current_position)", c=:green, ms=4, legend=:bottomleft)
        elseif (in(current_position, soft_wall_set) == true)
            scatter!([current_position[1]],[current_position[2]], label="", showaxis=:false, c=:gray69, ms=4)
        else
            scatter!([current_position[1]],[current_position[2]], label="", showaxis=:false, msc=:gray50, c=:white)
        end
    end
    xlabel!("Number of Apples",fontsize=18)
    ylabel!("Number of Oranges",fontsize=18)
    current()
end



